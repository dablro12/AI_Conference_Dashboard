import os
import requests
import yaml
import json
import time
import re
import concurrent.futures
from datetime import datetime, timedelta, timezone
import pytz
from flask import Flask, render_template, jsonify
from dateutil import parser as date_parser

# ìŠ¤ì¼€ì¤„ëŸ¬
from apscheduler.schedulers.background import BackgroundScheduler
import atexit
import webbrowser
from threading import Timer

app = Flask(__name__)

# --- ì„¤ì • ---
GITHUB_REPO_API = "https://api.github.com/repos/ccfddl/ccf-deadlines/contents/conference"
DATA_FILE = "conferences_data.json"      # [ë³€ê²½] ì´ì œ ì—¬ê¸°ì—” RAW ë°ì´í„°ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
LOG_FILE = "update_log.json"

CATEGORY_CONFIG = {
    'AI': {'folder': 'AI', 'name': 'Artificial Intelligence'},
    'CG': {'folder': 'CG', 'name': 'Computer Graphics'}, 
    'CT': {'folder': 'CT', 'name': 'Computing Theory'},
    'DB': {'folder': 'DB', 'name': 'Database / Data Mining / IR'},
    'DS': {'folder': 'DS', 'name': 'Computer Arch / Storage'},
    'HI': {'folder': 'HI', 'name': 'Computer-Human Interaction'},
    'MX': {'folder': 'MX', 'name': 'Interdisciplinary / Emerging'},
    'SE': {'folder': 'SE', 'name': 'Software Engineering'},
    'SC': {'folder': 'SC', 'name': 'Network & Security'},
    'NW': {'folder': 'NW', 'name': 'Computer Networks'}
}

# =========================================================
# 1. [í•µì‹¬] ë‚ ì§œ ê³„ì‚° ë¡œì§ (Reusable Helper)
# =========================================================
def calculate_utc_deadline(raw_date_str, raw_tz_str):
    """
    RAW ë¬¸ìì—´(ë‚ ì§œ, íƒ€ì„ì¡´)ì„ ë°›ì•„ì„œ -> ê³„ì‚°ëœ UTC datetime ê°ì²´ë¥¼ ë°˜í™˜
    """
    try:
        if not raw_date_str or raw_date_str == 'TBA':
            return None

        # 1. ë‚ ì§œ íŒŒì‹±
        deadline_dt = date_parser.parse(raw_date_str)
        
        # 2. íƒ€ì„ì¡´ ì˜¤í”„ì…‹ ê³„ì‚°
        tz_str = str(raw_tz_str).upper() if raw_tz_str else 'UTC'
        offset_hours = 0
        
        if 'AOE' in tz_str: offset_hours = -12
        elif 'UTC' in tz_str or 'GMT' in tz_str:
            match = re.search(r'(?:UTC|GMT)\s?([+-]?\d+)', tz_str)
            offset_hours = int(match.group(1)) if match else 0
        elif 'PST' in tz_str: offset_hours = -8
        elif 'PDT' in tz_str: offset_hours = -7
        elif 'EST' in tz_str: offset_hours = -5
        elif 'EDT' in tz_str: offset_hours = -4
        elif 'JST' in tz_str or 'KST' in tz_str: offset_hours = 9
        
        # 3. Timezone ê°ì²´ ìƒì„± ë° ì ìš©
        tz_obj = timezone(timedelta(hours=offset_hours))
        
        if deadline_dt.tzinfo is None:
            deadline_aware = deadline_dt.replace(tzinfo=tz_obj)
        else:
            deadline_aware = deadline_dt.astimezone(tz_obj)
        
        # 4. UTCë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return deadline_aware.astimezone(pytz.utc)

    except Exception:
        return None

# =========================================================
# 2. ë°ì´í„° ìˆ˜ì§‘ ë° RAW ì €ì¥ (Extraction)
# =========================================================
def extract_conference_info(conf, sub_code, full_name):
    """
    YAMLì„ ë¶„ì„í•˜ì—¬ 'ê°€ì¥ ì ì ˆí•œ ë¼ìš´ë“œ'ì˜ RAW ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ì—¬ê¸°ì„œëŠ” ê³„ì‚°ëœ ë‚ ì§œë¥¼ ì €ì¥í•˜ì§€ ì•Šê³ , ì›ë³¸ ë¬¸ìì—´ì„ ì €ì¥í•©ë‹ˆë‹¤)
    """
    try:
        if not conf.get('sub'): conf['sub'] = sub_code
        confs_list = conf.get('confs', [])
        
        # ê¸°ë³¸ ê³¨ê²©
        entry = {
            "id": conf.get('title'),
            "title": conf.get('title'),
            "description": conf.get('description'),
            "sub": conf.get('sub'),
            "sub_name": full_name,
            "rank": conf.get('rank', {}).get('ccf', 'N'),
            # --- [ë³€ê²½] RAW ë°ì´í„° ì €ì¥ í•„ë“œ ---
            "raw_deadline": None,
            "raw_timezone": None,
            "raw_place": "TBA",
            "year": "N/A",
            # ----------------------------------
            "has_future_round": False # ë‚˜ì¤‘ì— í•„í„°ë§ì„ ìœ„í•œ í”Œë˜ê·¸
        }

        if not confs_list: return entry

        now = datetime.now(pytz.utc)
        
        # ì—¬ëŸ¬ ë¼ìš´ë“œ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ë¯¸ë˜ì˜ ë¼ìš´ë“œë¥¼ ì°¾ìŒ
        best_round = None
        
        for c in confs_list:
            timeline = c.get('timeline', [])
            raw_tz = c.get('timezone', 'UTC') # íƒ€ì„ì¡´ì€ conf ë ˆë²¨ì— ìˆìŒ
            
            for t in timeline:
                raw_date = t.get('deadline', 'TBA')
                
                # ê³„ì‚°ê¸°ë¥¼ ëŒë ¤ì„œ ë¯¸ë˜ì¸ì§€ í™•ì¸ (ì„ íƒì„ ìœ„í•´ ê³„ì‚°ì€ í•„ìš”í•¨)
                utc_dt = calculate_utc_deadline(raw_date, raw_tz)
                
                if utc_dt and utc_dt > now:
                    # ë¯¸ë˜ ë¼ìš´ë“œ ë°œê²¬! ì´ ì •ë³´ë¥¼ ì €ì¥ í›„ë³´ë¡œ ì„ ì •
                    entry["year"] = c.get('year')
                    entry["raw_deadline"] = raw_date  # <--- ë³€í™˜ ì•ˆ í•˜ê³  ê·¸ëŒ€ë¡œ ì €ì¥
                    entry["raw_timezone"] = raw_tz    # <--- ë³€í™˜ ì•ˆ í•˜ê³  ê·¸ëŒ€ë¡œ ì €ì¥
                    entry["raw_place"] = c.get('place', 'TBA')
                    entry["has_future_round"] = True
                    best_round = True
                    break # íƒ€ì„ë¼ì¸ ë£¨í”„ íƒˆì¶œ
            
            if best_round:
                break # ì—°ë„ ë£¨í”„ íƒˆì¶œ
        
        return entry

    except Exception:
        return None

def fetch_single_yaml(url, sub_code, full_name):
    try:
        resp = requests.get(url, timeout=10)
        if resp.status_code != 200: return []
        confs = yaml.safe_load(resp.text)
        if isinstance(confs, dict): confs = [confs]
        
        results = []
        for conf in confs:
            processed = extract_conference_info(conf, sub_code, full_name)
            if processed: results.append(processed)
        return results
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return []

def fetch_conference_data(force_refresh=False):
    # 1. (ê¸°ì¡´ ë¡œì§) ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹ˆë©´ ë¡œì»¬ íŒŒì¼ ìš°ì„  ì‚¬ìš©
    if not force_refresh and os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data: # ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë¦¬í„´
                    return data
        except Exception:
            pass

    print("ğŸŒ Fetching fresh data from GitHub...")
    all_data = []
    tasks = []
    headers = {"Accept": "application/vnd.github.v3+json", "Authorization": f"token {os.getenv('GITHUB_TOKEN', '')}"}
    
    # API ì œí•œ í™•ì¸ìš© í”Œë˜ê·¸
    rate_limit_hit = False

    for sub_code, config in CATEGORY_CONFIG.items():
        try:
            resp = requests.get(f"{GITHUB_REPO_API}/{config['folder']}", headers=headers)
            
            # [ì¶”ê°€] API ì œí•œ ê±¸ë ¸ëŠ”ì§€ í™•ì¸
            if resp.status_code == 403 or resp.status_code == 429:
                print(f"âŒ [Error] GitHub API Rate Limit Hit! (Category: {sub_code})")
                rate_limit_hit = True
                break
                
            if resp.status_code == 200:
                for f in resp.json():
                    if f['name'].endswith('.yml'):
                        tasks.append((f['download_url'], sub_code, config['name']))
        except Exception as e: 
            print(f"Error fetching list: {e}")

    # API ì œí•œì— ê±¸ë¦¬ì§€ ì•Šì•˜ì„ ë•Œë§Œ ìƒì„¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰
    if not rate_limit_hit and tasks:
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(fetch_single_yaml, u, s, n) for u, s, n in tasks]
            for f in concurrent.futures.as_completed(futures):
                all_data.extend(f.result())

    # 3. [í•µì‹¬ ìˆ˜ì •] ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ëª¨ì˜€ì„ ë•Œë§Œ ì €ì¥
    if all_data:
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        save_update_time()
        print(f"âœ… Successfully updated {len(all_data)} conferences.")
        return all_data
    
    else:
        # ì‹¤íŒ¨í–ˆê±°ë‚˜ ë¹ˆ ë°ì´í„°ë¼ë©´
        print("âš ï¸ Warning: Fetched data is empty. (Maybe Rate Limit?)")
        
        # ê¸°ì¡´ ë¡œì»¬ íŒŒì¼ì´ë¼ë„ ìˆìœ¼ë©´ ê·¸ê±¸ ë°˜í™˜í•´ì„œ í™”ë©´ì´ ë°±ì§€ê°€ ë˜ëŠ” ê²ƒ ë°©ì§€
        if os.path.exists(DATA_FILE):
            print("ğŸ”„ Falling back to existing local data...")
            try:
                with open(DATA_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return []
        
        return []

# ë¡œê·¸ ê´€ë ¨ í•¨ìˆ˜ (ê¸°ì¡´ ë™ì¼)
def get_last_update_time():
    if not os.path.exists(LOG_FILE): return None
    try:
        with open(LOG_FILE, 'r') as f:
            return datetime.fromisoformat(json.load(f).get("last_success", ""))
    except: return None

def save_update_time():
    try:
        with open(LOG_FILE, 'w') as f:
            json.dump({"last_success": datetime.now().isoformat()}, f)
    except: pass

def scheduled_update_job():
    last = get_last_update_time()
    if not last or (datetime.now() - last).days >= 7:
        print("â° Auto-updating conferences...")
        fetch_conference_data(force_refresh=True)

# =========================================================
# 3. API ì„œë¹™ (Serving & Calculation)
# =========================================================
@app.route('/api/conferences')
def get_conferences_api():
    """
    ì €ì¥ëœ RAW ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ, 
    API ì‘ë‹µì„ ì¤„ ë•Œ ì‹¤ì‹œê°„ìœ¼ë¡œ UTC ì‹œê°„ì„ ê³„ì‚°í•´ì„œ ë‚´ë ¤ì¤ë‹ˆë‹¤.
    """
    raw_data = fetch_conference_data() # ë©”ëª¨ë¦¬ or íŒŒì¼ ë¡œë“œ
    response_data = []
    
    now = datetime.now(pytz.utc)

    for item in raw_data:
        # 1. RAW ë°ì´í„° ì½ê¸°
        raw_date = item.get('raw_deadline')
        raw_tz = item.get('raw_timezone')
        
        # 2. ì‹¤ì‹œê°„ ê³„ì‚°
        utc_dt = calculate_utc_deadline(raw_date, raw_tz)
        
        # 3. í”„ë¡ íŠ¸ì—”ë“œìš© ê°ì²´ ìƒì„±
        # (ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œ ì½”ë“œì™€ í˜¸í™˜ë˜ë„ë¡ í•„ë“œëª… ë§¤í•‘)
        final_obj = {
            "id": item['id'],
            "title": item['title'],
            "description": item['description'],
            "sub": item['sub'],
            "sub_name": item['sub_name'],
            "rank": item['rank'],
            "place": item.get('raw_place', 'TBA'),
            "year": item.get('year'),
            
            # ê³„ì‚°ëœ ê²°ê³¼ ì£¼ì…
            "deadline": utc_dt.isoformat() if utc_dt else None,
            
            # ë¯¸ë˜ì¸ì§€ ì•„ë‹Œì§€ ìµœì¢… íŒë‹¨ (ì €ì¥ ì‹œì ê³¼ ì„œë¹™ ì‹œì ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            "is_active": (utc_dt > now) if utc_dt else False
        }
        
        response_data.append(final_obj)
        
    return jsonify(response_data)

def open_browser():
    """ì„œë²„ê°€ ì‹œì‘ëœ í›„ 1.5ì´ˆ ë’¤ì— ë¸Œë¼ìš°ì €ë¥¼ ì—½ë‹ˆë‹¤."""
    print("ğŸŒ Opening browser...")
    webbrowser.open_new("http://127.0.0.1:5000")

@app.route('/api/refresh')
def refresh_data():
    fetch_conference_data(force_refresh=True)
    return jsonify({"status": "success"})

@app.route('/')
def index(): return render_template('index.html')

if __name__ == '__main__':
    # 1. ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = BackgroundScheduler()
    scheduler.add_job(func=scheduled_update_job, trigger="cron", day_of_week='sat', hour=9)
    scheduler.start()
    atexit.register(lambda: scheduler.shutdown())
    
    # 2. [ì¶”ê°€] ë¸Œë¼ìš°ì € ìë™ ì‹¤í–‰ íƒ€ì´ë¨¸
    # ì„œë²„ê°€ ì™„ì „íˆ ì¼œì§ˆ ì‹œê°„ì„ ì£¼ê¸° ìœ„í•´ 1.5ì´ˆ ë”œë ˆì´ë¥¼ ì¤ë‹ˆë‹¤.
    # (debug=True ëª¨ë“œì—ì„œëŠ” ë¦¬ë¡œë” ë•Œë¬¸ì— ë‘ ë²ˆ ì—´ë¦´ ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ ë°©ì§€í•˜ë ¤ë©´ í™˜ê²½ë³€ìˆ˜ ì²´í¬ê°€ í•„ìš”í•˜ì§€ë§Œ ì¼ë‹¨ ê°„ë‹¨íˆ êµ¬í˜„í•©ë‹ˆë‹¤)
    Timer(1.5, open_browser).start()

    print("ğŸš€ Server started with Scheduler (Every Saturday 9:00 AM)")
    app.run(debug=True, host='0.0.0.0', port=5000)